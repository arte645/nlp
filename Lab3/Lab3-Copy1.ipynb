{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a582fb1a-e697-4e0b-be04-d1d250b70851",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Начало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fc96e537-049b-4d3a-94a6-b01733a3bf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_29656\\2311108680.py:27: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('1mcorpus/corpus.en_ru.1m.en', encoding='utf-8', on_bad_lines='skip', delimiter='/n', header=None, index_col=False).tail(50000)\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_29656\\2311108680.py:27: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv('1mcorpus/corpus.en_ru.1m.en', encoding='utf-8', on_bad_lines='skip', delimiter='/n', header=None, index_col=False).tail(50000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>950000</th>\n",
       "      <td>Ensuring you a high quality of service and amenities, Napoleon Hostel has 3 stars.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950001</th>\n",
       "      <td>One will be able to meet the Old Slavic god of marriage, Kolodiy, and each son-in-law can ride his beloved mother-in-law in the wagon or sleigh, to feed her with dumplings and pancakes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950002</th>\n",
       "      <td>Located 250 metres from the centre of Žabljak, Apartments Vuković is set in the Nature Park Durmitor and surrounded by untouched nature and woods.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950003</th>\n",
       "      <td>We also offer bog trips in ARGO amphibious vehicles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950004</th>\n",
       "      <td>Lübeck’s Main Station is a direct bus ride or a 15-minute walk away.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>Such actions are called interventions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>Kaliningrad, May 20, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>Urinary tract infections in pregnancy A.I. Dyadyk, A.E. Bagriy, N.F. Yarovaya, M.V. Homenko, D.B. Reznikov, Yu.V. Roschin, I.G. Bagriy Urinary tract infections (UTI) associated with pregnancy represents the important problem of obstetrics, urology and nephrology, caused by changes in clinical features UTI, approaches for assessment and treatment UTI and high risk of obstetrical, urologic and n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>The default is 8 days for Exchange Server 5.5 and 40 days for Exchange 2000 Server, for Exchange Server 2003, for Exchange Server 2007, and for Exchange Server 2010.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>23 The KWrite Handbook Tools → Encoding You can overwrite the default encoding set in Settings → Congure Editor... in the Open/Save page to set a different encoding for your current document.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                      0\n",
       "950000                                                                                                                                                                                                                                                                                                                               Ensuring you a high quality of service and amenities, Napoleon Hostel has 3 stars.\n",
       "950001                                                                                                                                                                                                                        One will be able to meet the Old Slavic god of marriage, Kolodiy, and each son-in-law can ride his beloved mother-in-law in the wagon or sleigh, to feed her with dumplings and pancakes.\n",
       "950002                                                                                                                                                                                                                                                               Located 250 metres from the centre of Žabljak, Apartments Vuković is set in the Nature Park Durmitor and surrounded by untouched nature and woods.\n",
       "950003                                                                                                                                                                                                                                                                                                                                                             We also offer bog trips in ARGO amphibious vehicles.\n",
       "950004                                                                                                                                                                                                                                                                                                                                             Lübeck’s Main Station is a direct bus ride or a 15-minute walk away.\n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                 ...\n",
       "999995                                                                                                                                                                                                                                                                                                                                                                           Such actions are called interventions.\n",
       "999996                                                                                                                                                                                                                                                                                                                                                                                        Kaliningrad, May 20, 2013\n",
       "999997  Urinary tract infections in pregnancy A.I. Dyadyk, A.E. Bagriy, N.F. Yarovaya, M.V. Homenko, D.B. Reznikov, Yu.V. Roschin, I.G. Bagriy Urinary tract infections (UTI) associated with pregnancy represents the important problem of obstetrics, urology and nephrology, caused by changes in clinical features UTI, approaches for assessment and treatment UTI and high risk of obstetrical, urologic and n...\n",
       "999998                                                                                                                                                                                                                                            The default is 8 days for Exchange Server 5.5 and 40 days for Exchange 2000 Server, for Exchange Server 2003, for Exchange Server 2007, and for Exchange Server 2010.\n",
       "999999                                                                                                                                                                                                                  23 The KWrite Handbook Tools → Encoding You can overwrite the default encoding set in Settings → Congure Editor... in the Open/Save page to set a different encoding for your current document.\n",
       "\n",
       "[50000 rows x 1 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "pd.set_option('display.width', 500)\n",
    "\n",
    "df = pd.read_csv('1mcorpus/corpus.en_ru.1m.en', encoding='utf-8', on_bad_lines='skip', delimiter='/n', header=None, index_col=False).tail(50000)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c8a53174-962c-4e4e-80fc-9c5588f4b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4f027343-e1af-4247-a97c-4c734925372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_29656\\698285661.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df2 = pd.read_csv('1mcorpus/corpus.en_ru.1m.ru', encoding='utf-8', on_bad_lines='skip', delimiter='/n', header=None, index_col=False).tail(50000)\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_29656\\698285661.py:1: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df2 = pd.read_csv('1mcorpus/corpus.en_ru.1m.ru', encoding='utf-8', on_bad_lines='skip', delimiter='/n', header=None, index_col=False).tail(50000)\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('1mcorpus/corpus.en_ru.1m.ru', encoding='utf-8', on_bad_lines='skip', delimiter='/n', header=None, index_col=False).tail(50000)\n",
    "\n",
    "df2.to_csv('ru.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90290342-157d-4e20-b833-04886f976513",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('ru.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9520420e-e435-459f-8888-fc70200a2e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>950000</th>\n",
       "      <td>Ensuring you a high quality of service and amenities, Napoleon Hostel has 3 stars.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950001</th>\n",
       "      <td>One will be able to meet the Old Slavic god of marriage, Kolodiy, and each son-in-law can ride his beloved mother-in-law in the wagon or sleigh, to feed her with dumplings and pancakes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950002</th>\n",
       "      <td>Located 250 metres from the centre of Žabljak, Apartments Vuković is set in the Nature Park Durmitor and surrounded by untouched nature and woods.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950003</th>\n",
       "      <td>We also offer bog trips in ARGO amphibious vehicles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950004</th>\n",
       "      <td>Lübeck’s Main Station is a direct bus ride or a 15-minute walk away.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>Such actions are called interventions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>Kaliningrad, May 20, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>Urinary tract infections in pregnancy A.I. Dyadyk, A.E. Bagriy, N.F. Yarovaya, M.V. Homenko, D.B. Reznikov, Yu.V. Roschin, I.G. Bagriy Urinary tract infections (UTI) associated with pregnancy represents the important problem of obstetrics, urology and nephrology, caused by changes in clinical features UTI, approaches for assessment and treatment UTI and high risk of obstetrical, urologic and n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>The default is 8 days for Exchange Server 5.5 and 40 days for Exchange 2000 Server, for Exchange Server 2003, for Exchange Server 2007, and for Exchange Server 2010.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>23 The KWrite Handbook Tools → Encoding You can overwrite the default encoding set in Settings → Congure Editor... in the Open/Save page to set a different encoding for your current document.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                      0\n",
       "950000                                                                                                                                                                                                                                                                                                                               Ensuring you a high quality of service and amenities, Napoleon Hostel has 3 stars.\n",
       "950001                                                                                                                                                                                                                        One will be able to meet the Old Slavic god of marriage, Kolodiy, and each son-in-law can ride his beloved mother-in-law in the wagon or sleigh, to feed her with dumplings and pancakes.\n",
       "950002                                                                                                                                                                                                                                                               Located 250 metres from the centre of Žabljak, Apartments Vuković is set in the Nature Park Durmitor and surrounded by untouched nature and woods.\n",
       "950003                                                                                                                                                                                                                                                                                                                                                             We also offer bog trips in ARGO amphibious vehicles.\n",
       "950004                                                                                                                                                                                                                                                                                                                                             Lübeck’s Main Station is a direct bus ride or a 15-minute walk away.\n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                 ...\n",
       "999995                                                                                                                                                                                                                                                                                                                                                                           Such actions are called interventions.\n",
       "999996                                                                                                                                                                                                                                                                                                                                                                                        Kaliningrad, May 20, 2013\n",
       "999997  Urinary tract infections in pregnancy A.I. Dyadyk, A.E. Bagriy, N.F. Yarovaya, M.V. Homenko, D.B. Reznikov, Yu.V. Roschin, I.G. Bagriy Urinary tract infections (UTI) associated with pregnancy represents the important problem of obstetrics, urology and nephrology, caused by changes in clinical features UTI, approaches for assessment and treatment UTI and high risk of obstetrical, urologic and n...\n",
       "999998                                                                                                                                                                                                                                            The default is 8 days for Exchange Server 5.5 and 40 days for Exchange 2000 Server, for Exchange Server 2003, for Exchange Server 2007, and for Exchange Server 2010.\n",
       "999999                                                                                                                                                                                                                  23 The KWrite Handbook Tools → Encoding You can overwrite the default encoding set in Settings → Congure Editor... in the Open/Save page to set a different encoding for your current document.\n",
       "\n",
       "[50000 rows x 1 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9b710175-2a7a-4ca8-afa2-8c8debfc52f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>950000</th>\n",
       "      <td>Ensuring you a high quality of service and amenities, Napoleon Hostel has 3 stars.</td>\n",
       "      <td>Napoleon Hostel, имеющий 3 звезды, обеспечивает высокий уровень обслуживания и удобств.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950001</th>\n",
       "      <td>One will be able to meet the Old Slavic god of marriage, Kolodiy, and each son-in-law can ride his beloved mother-in-law in the wagon or sleigh, to feed her with dumplings and pancakes.</td>\n",
       "      <td>Также народ сможет встретиться со старославянским богом брака Колодием, а каждый зять сможет покатать любимую тещу на телеге или санях, покормить ее варениками и блинчиками.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950002</th>\n",
       "      <td>Located 250 metres from the centre of Žabljak, Apartments Vuković is set in the Nature Park Durmitor and surrounded by untouched nature and woods.</td>\n",
       "      <td>Отель Enigma расположен в самом центре национального парка Дурмитор, в 1,2 км от центра города Жабляк.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950003</th>\n",
       "      <td>We also offer bog trips in ARGO amphibious vehicles.</td>\n",
       "      <td>Есть также возможность добраться до болотной деревни и на автомобиле-амфибии ARGO.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950004</th>\n",
       "      <td>Lübeck’s Main Station is a direct bus ride or a 15-minute walk away.</td>\n",
       "      <td>До главного железнодорожного вокзала Любека можно добраться от отеля на прямом автобусе или дойти за 15 минут.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>Such actions are called interventions.</td>\n",
       "      <td>Такие действия называются интервенциями.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>Kaliningrad, May 20, 2013</td>\n",
       "      <td>Калининград, 20 мая 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>Urinary tract infections in pregnancy A.I. Dyadyk, A.E. Bagriy, N.F. Yarovaya, M.V. Homenko, D.B. Reznikov, Yu.V. Roschin, I.G. Bagriy Urinary tract infections (UTI) associated with pregnancy represents the important problem of obstetrics, urology and nephrology, caused by changes in clinical features UTI, approaches for assessment and treatment UTI and high risk of obstetrical, urologic and n...</td>\n",
       "      <td>Сообщение 1 А.И. Дядык, А.Э. Багрий, Н.Ф. Яровая, М.В. Хоменко, Д.Б. Резников, Ю.В. Рощин, И.Г. Багрий В первом сообщении приведены данные относительно эпидемиологии и этиологии инфекций мочевыводящих путей (МВП) у беременных, кратко изложены вопросы физиологических изменений МВП в период беременности.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>The default is 8 days for Exchange Server 5.5 and 40 days for Exchange 2000 Server, for Exchange Server 2003, for Exchange Server 2007, and for Exchange Server 2010.</td>\n",
       "      <td>Значение по умолчанию составляет 8 дней для сервера Exchange Server 5.5 и 40 дней для серверов Exchange Server 2000, Exchange Server 2003, Exchange Server 2007 и Exchange Server 2010.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>23 The KWrite Handbook Tools → Encoding You can overwrite the default encoding set in Settings → Congure Editor... in the Open/Save page to set a different encoding for your current document.</td>\n",
       "      <td>12 Руководство пользователя KWrite Сервис → Кодировка Сервис → Конец строки Здесь можно указать кодировку для текущего документа.Используемую по умолчанию кодировку можно изменить в Настройка → Настроить редактор... в разделе Открытие/сохранение.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                      0  \\\n",
       "950000                                                                                                                                                                                                                                                                                                                               Ensuring you a high quality of service and amenities, Napoleon Hostel has 3 stars.   \n",
       "950001                                                                                                                                                                                                                        One will be able to meet the Old Slavic god of marriage, Kolodiy, and each son-in-law can ride his beloved mother-in-law in the wagon or sleigh, to feed her with dumplings and pancakes.   \n",
       "950002                                                                                                                                                                                                                                                               Located 250 metres from the centre of Žabljak, Apartments Vuković is set in the Nature Park Durmitor and surrounded by untouched nature and woods.   \n",
       "950003                                                                                                                                                                                                                                                                                                                                                             We also offer bog trips in ARGO amphibious vehicles.   \n",
       "950004                                                                                                                                                                                                                                                                                                                                             Lübeck’s Main Station is a direct bus ride or a 15-minute walk away.   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                 ...   \n",
       "999995                                                                                                                                                                                                                                                                                                                                                                           Such actions are called interventions.   \n",
       "999996                                                                                                                                                                                                                                                                                                                                                                                        Kaliningrad, May 20, 2013   \n",
       "999997  Urinary tract infections in pregnancy A.I. Dyadyk, A.E. Bagriy, N.F. Yarovaya, M.V. Homenko, D.B. Reznikov, Yu.V. Roschin, I.G. Bagriy Urinary tract infections (UTI) associated with pregnancy represents the important problem of obstetrics, urology and nephrology, caused by changes in clinical features UTI, approaches for assessment and treatment UTI and high risk of obstetrical, urologic and n...   \n",
       "999998                                                                                                                                                                                                                                            The default is 8 days for Exchange Server 5.5 and 40 days for Exchange 2000 Server, for Exchange Server 2003, for Exchange Server 2007, and for Exchange Server 2010.   \n",
       "999999                                                                                                                                                                                                                  23 The KWrite Handbook Tools → Encoding You can overwrite the default encoding set in Settings → Congure Editor... in the Open/Save page to set a different encoding for your current document.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                      1  \n",
       "950000                                                                                                                                                                                                                          Napoleon Hostel, имеющий 3 звезды, обеспечивает высокий уровень обслуживания и удобств.  \n",
       "950001                                                                                                                                    Также народ сможет встретиться со старославянским богом брака Колодием, а каждый зять сможет покатать любимую тещу на телеге или санях, покормить ее варениками и блинчиками.  \n",
       "950002                                                                                                                                                                                                           Отель Enigma расположен в самом центре национального парка Дурмитор, в 1,2 км от центра города Жабляк.  \n",
       "950003                                                                                                                                                                                                                               Есть также возможность добраться до болотной деревни и на автомобиле-амфибии ARGO.  \n",
       "950004                                                                                                                                                                                                   До главного железнодорожного вокзала Любека можно добраться от отеля на прямом автобусе или дойти за 15 минут.  \n",
       "...                                                                                                                                                                                                                                                                                                                 ...  \n",
       "999995                                                                                                                                                                                                                                                                         Такие действия называются интервенциями.  \n",
       "999996                                                                                                                                                                                                                                                                                         Калининград, 20 мая 2013  \n",
       "999997  Сообщение 1 А.И. Дядык, А.Э. Багрий, Н.Ф. Яровая, М.В. Хоменко, Д.Б. Резников, Ю.В. Рощин, И.Г. Багрий В первом сообщении приведены данные относительно эпидемиологии и этиологии инфекций мочевыводящих путей (МВП) у беременных, кратко изложены вопросы физиологических изменений МВП в период беременности.  \n",
       "999998                                                                                                                          Значение по умолчанию составляет 8 дней для сервера Exchange Server 5.5 и 40 дней для серверов Exchange Server 2000, Exchange Server 2003, Exchange Server 2007 и Exchange Server 2010.  \n",
       "999999                                                           12 Руководство пользователя KWrite Сервис → Кодировка Сервис → Конец строки Здесь можно указать кодировку для текущего документа.Используемую по умолчанию кодировку можно изменить в Настройка → Настроить редактор... в разделе Открытие/сохранение.  \n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, df2],axis=1, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2124bc-5a27-4022-ab12-ab30011b8c0f",
   "metadata": {},
   "source": [
    "## Кодировщик-декодеровщик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1908cc80-464f-411f-b019-115d02592763",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sents = open('en.csv', encoding='utf-8').read().splitlines()[:1000]\n",
    "ru_sents = open('ru.csv', encoding='utf-8').read().splitlines()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd453578-c102-46b1-a15a-1100403f1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokenizer_en = Tokenizer(BPE())\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "trainer_en = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer_en.train(files=[\"en.csv\"], trainer=trainer_en)\n",
    "\n",
    "tokenizer_ru = Tokenizer(BPE())\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "trainer_ru = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer_ru.train(files=[\"ru.csv\"], trainer=trainer_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dacbd70e-4f0a-4aa2-8839-eb85b170fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en.save('tokenizer_en')\n",
    "tokenizer_ru.save('tokenizer_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0bf16b-cb8a-47a7-9450-7629265c30a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(text, tokenizer, max_len):\n",
    "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[SEP]')]\n",
    "\n",
    "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')\n",
    "tokenizer_en.token_to_id('[PAD]')\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3804e937-f156-49b9-90ec-1bad36d0317f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ru.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be8b1f9b-4f04-4808-b688-ae7a673d16cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29978a76-3359-453e-8f90-34e4df01d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_en, max_len_ru = 40, 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc9b369c-1e7c-4d0f-a21f-a55b097103dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en, max_len_en) for t in en_sents]\n",
    "X_ru = [encode(t, tokenizer_ru, max_len_ru) for t in ru_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75c0ddd6-4624-406c-a724-8ce5e57bb1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_en), len(X_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec928a0b-8f47-4301-a5ff-10d7929e63da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4773,\n",
       " 1340,\n",
       " 21,\n",
       " 6218,\n",
       " 35,\n",
       " 248,\n",
       " 1566,\n",
       " 1646,\n",
       " 592,\n",
       " 2532,\n",
       " 482,\n",
       " 1228,\n",
       " 252,\n",
       " 495,\n",
       " 290,\n",
       " 300,\n",
       " 469,\n",
       " 22693,\n",
       " 5890,\n",
       " 1917,\n",
       " 23,\n",
       " 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ru[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c77e4f07-3f57-440d-b649-6a522f10dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "ru_tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "\n",
    "# Токенизация и паддинг\n",
    "X = tokenizer(en_sents, padding=True, return_tensors=\"pt\")\n",
    "Y = ru_tokenizer(ru_sents, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "086cc774-972e-4f91-be4f-aabe78355ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1010,  1014,  ...,     0,     0,     0],\n",
       "        [  101, 20317,  8889,  ...,     0,     0,     0],\n",
       "        [  101, 20317,  8889,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101, 20317,  2683,  ...,     0,     0,     0],\n",
       "        [  101, 20317,  2683,  ...,     0,     0,     0],\n",
       "        [  101, 20317,  2683,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f0e9d52-21c9-4b76-b2fe-f95b207ac466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_35232\\849731593.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(x, dtype=torch.int64) for x in X['input_ids']\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_35232\\849731593.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(x, dtype=torch.int64) for x in Y['input_ids']\n"
     ]
    }
   ],
   "source": [
    "X = nn.utils.rnn.pad_sequence([\n",
    "    torch.tensor(x, dtype=torch.int64) for x in X['input_ids']\n",
    "], padding_value=tokenizer_en.get_vocab_size()).T\n",
    "Y = nn.utils.rnn.pad_sequence([\n",
    "    torch.tensor(x, dtype=torch.int64) for x in Y['input_ids']\n",
    "], padding_value=tokenizer_ru.get_vocab_size()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f71d23b0-c27f-432b-88b9-8a6b603533c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([990, 134]),\n",
       " torch.Size([990, 165]),\n",
       " torch.Size([10, 134]),\n",
       " torch.Size([10, 165]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.01)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e180490-ae4c-44c3-9985-eb2c96d68827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Гиперпараметры\n",
    "INPUT_DIM = tokenizer.vocab_size  # Входная длина (source sequence)\n",
    "OUTPUT_DIM = ru_tokenizer.vocab_size  # Выходная длина (target sequence)\n",
    "EMBEDDING_DIM = 256  # Размерность embeddings\n",
    "HIDDEN_DIM = 512  # Размер скрытого слоя GRU\n",
    "BATCH_SIZE = 50  # Размер батча\n",
    "LEARNING_RATE = 0.001  # Скорость обучения\n",
    "N_EPOCHS = 2  # Количество эпох\n",
    "TEACHER_FORCING_RATIO = 0.5  # Коэффициент teacher forcing\n",
    "\n",
    "# Кодировщик\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)  # input_dim = INPUT_DIM\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)  # Преобразование индексов в векторы\n",
    "        outputs, hidden = self.rnn(embedded)  # RNN обработка\n",
    "        return hidden\n",
    "# Декодер\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # input = [batch_size], hidden = [1, batch_size, hidden_dim]\n",
    "        input = input.unsqueeze(1)  # input = [batch_size, 1]\n",
    "        embedded = self.embedding(input)  # embedded = [batch_size, 1, embedding_dim]\n",
    "        output, hidden = self.rnn(embedded, hidden)  # output = [batch_size, 1, hidden_dim]\n",
    "        prediction = self.fc_out(output.squeeze(1))  # prediction = [batch_size, output_dim]\n",
    "        return prediction, hidden\n",
    "\n",
    "# Seq2Seq модель\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len = trg.size(1)\n",
    "        batch_size = trg.size(0)\n",
    "        outputs = torch.zeros(batch_size, trg_len, OUTPUT_DIM).to(trg.device)\n",
    "        hidden = self.encoder(src)  # Начальное скрытое состояние от кодировщика\n",
    "        input = trg[:, 0]  # Начинаем с первого токена (SOS)\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)  # Извлекаем токен с наибольшей вероятностью\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27bdc493-9d7c-4517-a465-5d43183d948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "model = Seq2Seq(encoder, decoder).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Оптимизатор и функция потерь\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d754ca8a-4202-41de-ba8e-9d20a3ca59a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лосс с батча: 11.691032409667969\n",
      "Лосс с батча: 23.237709999084473\n",
      "Лосс с батча: 34.61364936828613\n",
      "Лосс с батча: 45.857863426208496\n",
      "Лосс с батча: 56.74956130981445\n",
      "Лосс с батча: 67.17886066436768\n",
      "Лосс с батча: 77.13906955718994\n",
      "Лосс с батча: 86.597731590271\n",
      "Лосс с батча: 95.68481922149658\n",
      "Лосс с батча: 104.46657276153564\n",
      "Лосс с батча: 113.0367841720581\n",
      "Лосс с батча: 121.43892860412598\n",
      "Лосс с батча: 129.68616008758545\n",
      "Лосс с батча: 138.14119625091553\n",
      "Лосс с батча: 146.43154048919678\n",
      "Лосс с батча: 154.65403079986572\n",
      "Лосс с батча: 163.12715530395508\n",
      "Лосс с батча: 171.46352672576904\n",
      "Лосс с батча: 179.9744644165039\n",
      "Лосс с батча: 188.27903652191162\n",
      "----------------------\n",
      "Epoch [1/2], Loss: 188.2790\n",
      "----------------------\n",
      "Лосс с батча: 7.580845355987549\n",
      "Лосс с батча: 14.971240043640137\n",
      "Лосс с батча: 22.20725965499878\n",
      "Лосс с батча: 29.53685474395752\n",
      "Лосс с батча: 36.65718746185303\n",
      "Лосс с батча: 43.67736530303955\n",
      "Лосс с батча: 50.59672689437866\n",
      "Лосс с батча: 57.5425238609314\n",
      "Лосс с батча: 64.62341165542603\n",
      "Лосс с батча: 71.62004518508911\n",
      "Лосс с батча: 78.58905601501465\n",
      "Лосс с батча: 85.66472101211548\n",
      "Лосс с батча: 92.64428377151489\n",
      "Лосс с батча: 99.8801121711731\n",
      "Лосс с батча: 106.97914743423462\n",
      "Лосс с батча: 114.08784532546997\n",
      "Лосс с батча: 121.38240242004395\n",
      "Лосс с батча: 128.61415719985962\n",
      "Лосс с батча: 136.0154047012329\n",
      "Лосс с батча: 143.21822500228882\n",
      "----------------------\n",
      "Epoch [2/2], Loss: 143.2182\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, X_train.size(0), BATCH_SIZE):\n",
    "        src = X_train[i:i + BATCH_SIZE].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        trg = y_train[i:i + BATCH_SIZE].to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio=TEACHER_FORCING_RATIO)\n",
    "        \n",
    "        # Убираем первый токен и приводим к [batch_size * trg_len, output_dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f'Лосс с батча: {epoch_loss}')\n",
    "    print('----------------------')\n",
    "    print(f'Epoch [{epoch+1}/{N_EPOCHS}], Loss: {epoch_loss:.4f}')\n",
    "    print('----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f543282c-07db-4208-bcd0-1cd164ac904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "output = model(X_test, y_test, teacher_forcing_ratio=TEACHER_FORCING_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9366740-ce0f-47d5-b9f7-5d84ca6f4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = output.shape[-1]\n",
    "output = output[:, 1:].reshape(-1, output_dim)\n",
    "trg = y_test[:, 1:].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89239261-73dc-4d13-92ef-cace97b7b261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 30509,   137,  ...,   102,   132,   108],\n",
       "        [    0, 30509,   137,  ...,   102,   132,   108],\n",
       "        [    0, 30509,   137,  ...,   102,   132,   108],\n",
       "        ...,\n",
       "        [    0, 30509,   137,  ...,   102,   132,   108],\n",
       "        [    0, 30509,   137,  ...,   102,   132,   108],\n",
       "        [    0, 30509,   137,  ...,   102,   132,   108]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = output.softmax(2).argmax(2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac4496-0a9d-4c13-8542-dc02ac866c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fd0a39b-159b-4e19-95e6-9f3f63ddf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_en_sents = [ru_tokenizer.decode(ids, skip_special_tokens=True) for ids in res[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b657476-8971-4c2e-8f88-86aef2ee79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_ru_sents = [ru_tokenizer.decode(ids, skip_special_tokens=True) for ids in trg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27833a36-8bef-4ca3-9dc5-ec315b62cd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-5.1773, -5.0126, -4.8821,  ..., -4.6691, -3.8730, -4.7070],\n",
       "         [-7.9525, -7.2474, -6.6537,  ..., -6.9330, -5.9149, -6.7827],\n",
       "         ...,\n",
       "         [-8.2891, -8.7641, -7.9166,  ..., -7.7674, -7.0388, -7.2581],\n",
       "         [-7.1868, -7.9089, -7.1350,  ..., -6.8353, -6.4027, -6.7858],\n",
       "         [-7.2471, -8.0015, -7.2900,  ..., -6.9146, -6.9417, -6.5111]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-5.1774, -5.0124, -4.8819,  ..., -4.6693, -3.8730, -4.7070],\n",
       "         [-7.9525, -7.2474, -6.6537,  ..., -6.9330, -5.9149, -6.7826],\n",
       "         ...,\n",
       "         [-8.2901, -8.7646, -7.9160,  ..., -7.7676, -7.0377, -7.2573],\n",
       "         [-7.1879, -7.9094, -7.1344,  ..., -6.8357, -6.4016, -6.7850],\n",
       "         [-7.2484, -8.0021, -7.2896,  ..., -6.9150, -6.9408, -6.5108]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-5.1773, -5.0125, -4.8820,  ..., -4.6692, -3.8730, -4.7070],\n",
       "         [-7.9525, -7.2474, -6.6537,  ..., -6.9330, -5.9149, -6.7826],\n",
       "         ...,\n",
       "         [-8.2900, -8.7647, -7.9177,  ..., -7.7674, -7.0399, -7.2594],\n",
       "         [-7.1877, -7.9094, -7.1361,  ..., -6.8354, -6.4038, -6.7871],\n",
       "         [-7.2480, -8.0020, -7.2911,  ..., -6.9147, -6.9428, -6.5124]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-5.1774, -5.0125, -4.8820,  ..., -4.6692, -3.8730, -4.7070],\n",
       "         [-7.9525, -7.2474, -6.6537,  ..., -6.9330, -5.9149, -6.7826],\n",
       "         ...,\n",
       "         [-8.2899, -8.7646, -7.9176,  ..., -7.7674, -7.0398, -7.2593],\n",
       "         [-7.1876, -7.9093, -7.1360,  ..., -6.8354, -6.4037, -6.7869],\n",
       "         [-7.2479, -8.0020, -7.2910,  ..., -6.9147, -6.9427, -6.5123]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-5.1773, -5.0125, -4.8821,  ..., -4.6691, -3.8730, -4.7070],\n",
       "         [-7.9525, -7.2474, -6.6537,  ..., -6.9330, -5.9149, -6.7827],\n",
       "         ...,\n",
       "         [-8.2902, -8.7648, -7.9176,  ..., -7.7675, -7.0398, -7.2593],\n",
       "         [-7.1878, -7.9095, -7.1360,  ..., -6.8355, -6.4036, -6.7870],\n",
       "         [-7.2482, -8.0022, -7.2910,  ..., -6.9148, -6.9426, -6.5123]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-5.1773, -5.0125, -4.8821,  ..., -4.6691, -3.8730, -4.7070],\n",
       "         [-7.9525, -7.2474, -6.6537,  ..., -6.9330, -5.9149, -6.7827],\n",
       "         ...,\n",
       "         [-8.2887, -8.7638, -7.9159,  ..., -7.7673, -7.0383, -7.2575],\n",
       "         [-7.1863, -7.9086, -7.1344,  ..., -6.8353, -6.4021, -6.7851],\n",
       "         [-7.2466, -8.0012, -7.2894,  ..., -6.9146, -6.9412, -6.5104]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc4ac71a-dc42-4d95-91da-893c0d9e91b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9500,,,,,,,,,,,.,,,,,,,,,.,,,,,,. \",.,. \",,.,,,.,.,. \",.,,.,.,,. \",,,.,,,,,.,,,,. \",. \",,. \",,. \".,. \".,. \",. \",,,. \",. \".,.,,,,,,. \",. \",.. \",,,. \",.,.,,. \".,,. \". \". \". \". \". \"', '9500,,,,,,,,,,,,,,,,,,,,,.,,,.,,,. \". \". \",,.,,,.,,. \",.,,.,.,,. \",,,.,,,,,.,,,,. \",. \",,. \",,. \".,. \".,. \",. \",,,. \",. \".,.,,,,,,. \",. \". \". \",,,. \",.. \",,. \".,,. \". \". \". \". \". \"', '9500,,,,,,,,,,,,,,,,,,,,,,,,,,,,. \",.,. \",,.,,,.,.,. \",.,,.,.,,. \",,,.,,,,,.,,,,. \",. \",,. \",,. \".,. \".,. \",. \",,,. \",. \".,.,,,,,,. \",. \",.. \",,,. \",.,.,,. \".,,. \". \". \". \". \". \"', '9500,,,,,,,,,,,,,,,,,,,,,.,,,.,,. \",.,. \".,..,,.,. \",.,,.,.,,. \",,,.,,,,,.,,,,. \",. \",,. \",,. \".,. \".,. \",. \",,,. \",. \".,.,,,,,,. \",. \". \". \",,,. \",.. \",,. \".,,. \". \". \". \". \". \"', '9500,,,,,,,,,,,,,,,,,,,,,,,,. \",,. \". \". \",,..,,.,.. \",,. \",,.,,. \",,,.,,,,,.,,,,. \",. \",,. \",,. \".,. \".,. \",. \",,,. \",. \".,.,,,,,,. \",. \". \". \",,,. \",.. \",,. \".,,. \". \". \". \". \". \"', '9500,,,,,,,,,,,,,,,,,,,,,.,,,,,,. \",,,,.,,.,.,.,.,. \",..,.,.,,. \",,,.,,,,,.,,,,. \",. \",,. \",,. \".,. \".,. \",. \",,,. \",. \".,.,,,,,,. \",. \". \". \",,,. \",.. \",,. \".,,. \". \". \". \". \". \"', '9500,,,,,,,,,,,,,,,,,,,,,.,,,,,,. \",.,. \",,.,,,.,.,. \",.,,.,.,,. \",,,.,,,,,.,,,,. \",. \",,. \",,. \".,. \".,. \",. \",,,. \",. \".,.,,,,,,. \",. \",.. \",,,. \",.,.,,. \".,,. \". \". \". \". \". \"', '9500,,,,,,,,,,,,,,,,,,,,,,,,,,,,. \",.,. \",,.,,,.,.,. \",.,,.,.,,. \",,,.,,,,,.,,,,. \",. \",,. \",,. \".,. \".,. \",. \",,,. \",. \".,.,,,,,,. \",. \",.. \",,,. \",.,.,,. \".,,. \". \". \". \". \". \"', '9500,,,,,,,,,,,,,,,,,,,,.,,,., \",.,.,. \",,.,,,.,.,. \",.,,.,.,,. \",,,.,,,,,.,,,,. \",. \",,. \",,. \".,. \".,. \",. \",,,. \",. \".,.,,,,,,. \",. \",.. \",,,. \",.,.,,. \".,,. \". \". \". \". \". \"', '9500,,,,,,,,,,,,,,,,,,,,,.,,,,,,. \",.,. \",,.,,,.,.,. \",.,,.,.,,. \",,,.,,,,,.,,,,. \",. \",,. \",,. \".,. \".,. \",. \",,,. \",. \".,.,,,,,,. \",. \",.. \",,,. \",.,.,,. \".,,. \". \". \". \". \". \"']\n"
     ]
    }
   ],
   "source": [
    "print(decoded_en_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfb02952-67c6-41bb-8e43-0ff35736d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['950', '##59', '##5', ',', '\"', 'Так', 'же', ',', 'как', 'флаг', ',', 'герб', 'и', 'гимн', '.', '\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '950', '##39', '##1', ',', '\"', 'На', 'сцене', 'менеджер', 'продукта', 'проводил', 'презентацию', ',', 'говоря', 'обычную', 'чеп', '##уху', 'со', 'словами', '«', 'потряс', '##ающий', '»', 'и', '«', 'фантастический', '»', 'и', 'другими', 'хвалеб', '##ными', 'словес', '##ами', ',', 'которыми', 'так', 'любят', 'сор', '##ить', 'сотрудники', 'Microsoft', 'на', 'презентация', '##х', '.', '\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '950', '##0', '##69', ',', '\"', 'В', 'этом', 'разделе', 'объясняется', ',', 'как', 'использовать', 'тестов', '##ый', 'телефон', 'единой', 'системы', 'обмена', 'сообщениями', 'Exchange', 'для', 'проверки', 'функций', 'абонент', '##ского', 'доступа', 'в', 'единой', 'системе', 'обмена', 'сообщениями', 'Microsoft', 'Exchange', 'Server', '2007', '.', '\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '950', '##36', '##7', ',', '\"', 'Суть', 'ее', 'состоит', 'в', 'том', ',', 'что', 'свое', 'первое', 'значение', 'слово', 'получает', ',', 'когда', 'человек', 'впервые', 'сталкивается', 'с', 'ним', ',', 'а', 'затем', '\"', '\"', 'большинство', 'слов', ',', 'переходя', 'из', 'контекста', 'в', 'контекст', ',', 'меняют', 'свое', 'значение', '\"', '\"', '(', 'Richard', '##s', ')', '.', '\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '950', '##01', '##9', ',', '\"', 'И', 'затем', 'в', 'конце', 'тяжёлого', 'испытания', ',', 'когда', 'Он', 'увидел', ',', 'что', 'Его', 'Сын', 'занимался', 'делами', 'Отца', ';', 'Гора', 'Преображения', ',', 'Книга', 'Луки', ';', 'Он', 'взял', 'Петра', ',', 'Иакова', 'и', 'Иоанна', ',', 'троих', 'свидетелей', ',', 'и', 'поднялся', 'на', 'вершину', 'горы', ',', 'и', 'там', 'Бог', 'исполнил', 'закон', 'усыновления', '.', '\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '950', '##40', '##7', ',', '\"', 'Обслуж', '##ивание', 'номеров', ',', 'Ня', '##ня', '/', 'Услуги', 'по', 'уходу', 'за', 'детьми', ',', 'Прок', '##ат', 'автомобилей', ',', 'Экск', '##урс', '##ионное', 'бюро', ',', 'Услуги', 'конс', '##ьер', '##жа', ',', 'Транс', '##фер', '(', 'за', 'дополнительную', 'плату', ')', ',', 'Специальные', 'диет', '##ические', 'меню', '(', 'по', 'запросу', ')', ',', 'Транс', '##фер', 'от', '/', 'до', 'аэропорта', '(', 'платный', ')', '\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '950', '##16', '##6', ',', 'Коллекция', 'заданий', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '950', '##89', '##2', ',', '\"', 'Данные', 'с', 'внешних', 'входов', 'могут', 'отображать', '##ся', 'на', 'дисп', '##лее', ',', 'отправляться', 'на', 'аналогов', '##ый', 'выход', 'и', '/', 'или', 'цифровой', 'интерфейс', 'RS', '##23', '##2', '.', '\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '950', '##01', '##7', ',', '\"', 'Пожалуй', '##ста', ',', 'обрат', '##ите', 'внимание', ',', 'что', 'в', 'стоимость', 'проживания', 'по', 'системе', '\"', '\"', 'полуп', '##анси', '##он', '\"', '\"', 'не', 'включены', 'напитки', '.', '\"', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '950', '##0', '##60', ',', 'Следует', 'учитывать', 'следующее', ':', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(decoded_ru_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6dc70346-def4-4829-b60e-080a13cef662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##0'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_tokenizer.decode(137, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97172b-7a26-4eb9-baf2-7303c5e78718",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## бибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bd5b963-2688-4d49-8a0e-d88c0bcb596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(X_train, y_train)\n",
    "training_generator = torch.utils.data.DataLoader(training_set, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05acb630-5240-4c99-b578-e2bd55da8190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5344, 0.1966, 0.1966, 0.0723]),\n",
       " tensor([0.0723, 0.1966, 0.1966, 0.5344]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_size_in: int,\n",
    "        hidden_size_out: int,\n",
    "        dropout: float,\n",
    "        rnn_num_layers: int = 1,\n",
    "        bidirectional: bool = False,\n",
    "    ) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size_in = hidden_size_in\n",
    "        self.hidden_size_out = hidden_size_out\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = rnn_num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=input_dim,  # vocab_size\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size_in,\n",
    "            num_layers=rnn_num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(2 * hidden_size_in, hidden_size_out)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "            inputs ~ [batch_size, padded_input_doc_len_in_batch]\n",
    "\n",
    "            output ~ Size([batch_size, padded_input_doc_len_in_batch, bi * hidden_size_in])\n",
    "            h_n ~ Size([bi * rnn_num_layers, batch_size, hidden_size_in])\n",
    "            h_cat ~ Size([batch_size, bi * hidden_size_in])\n",
    "            \n",
    "            output ~ Size([batch_size, padded_input_doc_len_in_batch, hidden_size_out])\n",
    "            hidden ~ Size([batch_size, hidden_size_out])\n",
    "\n",
    "            encoder h_n -> decoder h_0\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(inputs)  #.flip(1)  # [batch_size, padded_input_doc_len_in_batch, embedding_dim]\n",
    "        output, h_n = self.gru(embedded)\n",
    "        h_cat = torch.cat([h_n[0, :, :], h_n[1, :, :]], dim=1)  # если bi=1 и rnn_num_layers != 1 ?\n",
    "        hidden = torch.tanh(self.fc(h_cat))\n",
    "        output = torch.tanh(self.fc(output))\n",
    "        return output, hidden\n",
    "h_n = torch.ones(2, 20, 32)\n",
    "torch.cat([h_n[0, :, :], h_n[1, :, :]], dim=1).shape\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size_e: int,\n",
    "        hidden_size_d: int,\n",
    "    ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size_e = hidden_size_e\n",
    "        self.hidden_size_d = hidden_size_d\n",
    "        self.fc1 = nn.Linear(2 * hidden_size_d, hidden_size_d)\n",
    "        self.fc2 = nn.Linear(hidden_size_d, 1, bias=False)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        outputs_e: torch.Tensor,\n",
    "        hidden_d: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            -outputs_e ~ Size([batch_size, input_doc_len, bi * hidden_size_e])\n",
    "            +outputs_e ~ Size([batch_size, input_doc_len, hidden_size_d])\n",
    "            hidden_d ~ Size([batch_size, hidden_size_d])\n",
    "            \n",
    "            return: Size([batch_size, doc_len])\n",
    "        \"\"\"\n",
    "        # Реализация со скалярными произведениями\n",
    "        # Если че тут бахнуть слой с активацией 🤯\n",
    "        hidden = hidden_d.unsqueeze(1).repeat(1, outputs_e.shape[1], 1)\n",
    "        cat = torch.cat([hidden, outputs_e], dim=2)\n",
    "        attention = torch.tanh(self.fc1(cat))\n",
    "        attention = self.fc2(attention).squeeze(dim=2)        \n",
    "        distributions = torch.softmax(attention, dim=1)\n",
    "        \n",
    "#         hidden_d = hidden_d.unsqueeze(2)  # [batch_size, hidden_size_d, 1]\n",
    "#         scores = torch.bmm(outputs_e, hidden_d)  # [batch_size, input_doc_len, 1]\n",
    "#         scores = scores.squeeze(2)  # [batch_size, input_doc_len]\n",
    "#         distributions = scores.softmax(1)  # [batch_size, input_doc_len]\n",
    "        return distributions\n",
    "a = torch.ones(20, 7, 8)\n",
    "b = torch.ones(20, 8)\n",
    "bb = b.unsqueeze(1).repeat(1, 7, 1)\n",
    "torch.cat([bb, a], dim=2).shape\n",
    "# outputs_e ~ Size([batch_size, input_doc_len, bi * hidden_size_e])\n",
    "# hidden_d ~ Size([1, batch_size, hidden_size_d])\n",
    "a = torch.ones(20, 7, 8)\n",
    "b = torch.ones(1, 20, 8)\n",
    "bb = b.permute(1, 2, 0)  # [20, 8, 1]\n",
    "torch.bmm(a, bb).squeeze(2).softmax(1)  # [20, 8]\n",
    "\n",
    "# bb = b.permute(1, 0, 2).repeat(1, 7, 1)  # [20, 7, 8]\n",
    "# torch.cat([bb, a], dim=2).shape  # [20, 7, 2*8 + 8]\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim: int,\n",
    "        embedding_dim: int,\n",
    "        hidden_size_e: int,\n",
    "        hidden_size: int,\n",
    "        attention: Attention,\n",
    "        dropout: float,\n",
    "        rnn_num_layers: int = 1,\n",
    "    ) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = rnn_num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=output_dim,  # макс кол-во векторов = vocab_size\n",
    "            embedding_dim=embedding_dim,  # заданной размерности\n",
    "        )\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=rnn_num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        hidden: torch.Tensor,\n",
    "        encoder_outputs: torch.Tensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            input: batch of sos ~ [batch_size]\n",
    "            hidden ~ [batch_size, hidden_size_d]\n",
    "            encoder_outputs ~ [batch_size, input_doc_len, hidden_size_d]\n",
    "        \"\"\"\n",
    "        attention_distributes = self.attention(\n",
    "            outputs_e=encoder_outputs,\n",
    "            hidden_d=hidden,\n",
    "        )  # [batch_size, input_doc_len]\n",
    "        context = torch.bmm(\n",
    "            attention_distributes.unsqueeze(1),  # [batch_size, 1, input_doc_len]\n",
    "            encoder_outputs,  # [batch_size, input_doc_len, hidden_size_d]\n",
    "        )  # [batch_size, 1, hidden_size_d]\n",
    "        context = context.permute(1, 0, 2)  # [1, batch_size, hidden_size_d]\n",
    "\n",
    "        inputs = input.unsqueeze(1)  # [batch_size, 1]\n",
    "        embedded = self.embedding(inputs)  # [batch_size, 1, embedding_dim]\n",
    "\n",
    "        # output ~ [batch_size, 1, hidden_size_d]\n",
    "        # h_n ~ [1 * rnn_num_layers, batch_size, hidden_size_d]\n",
    "        output, h_n = self.gru(embedded, context)\n",
    "\n",
    "        prediction = self.fc(h_n.squeeze(0))  # [batch_size, output_size]\n",
    "        h_n = h_n.squeeze(0)  # [batch_size, hidden_size_d]\n",
    "        return prediction, h_n\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: Encoder,\n",
    "        decoder: Decoder,\n",
    "        device,\n",
    "    ) -> None:\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "#         assert encoder.hidden_size == decoder.hidden_size, 'hidden_size of Encoder and Decoder did not match'\n",
    "        assert encoder.n_layers == decoder.n_layers, 'n_layers of Encoder and Decoder did not match'\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs,\n",
    "        target_outputs,\n",
    "        teacher_forcing_ratio: float = 0.,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            inputs ~ [batch_size, padded_in_batch_input_doc_len]\n",
    "            target_outputs ~ [batch_size, padded_in_batch_target_doc_len]\n",
    "        \"\"\"\n",
    "        batch_size, max_len = target_outputs.shape\n",
    "        target_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        target_pad = 0\n",
    "        target_sos = 1\n",
    "        \n",
    "        # outputs ~ [max_len, batch_size, target_vocab_size]\n",
    "        outputs = torch.empty(max_len, batch_size, target_vocab_size, device=self.device).fill_(target_sos)\n",
    "\n",
    "        # encoder_outputs ~ [batch_size, input_doc_len, hidden_size_d]\n",
    "        # hidden ~ [batch_size, hidden_size_d]\n",
    "        encoder_outputs, hidden = self.encoder(inputs)\n",
    "\n",
    "#         input = torch.empty(batch_size, device=self.device).fill_(target_sos).type(torch.int64)\n",
    "        input = target_outputs[:, 0]  # batch of sos ~ [batch_size]\n",
    "        outputs[0, :, target_sos] = 1\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            # pred ~ [batch_size, target_vocab_size]\n",
    "            # hidden ~ [1 * rnn_num_layers, batch_size, hidden_size_d]\n",
    "            pred, hidden = self.decoder(\n",
    "                input=input,  # [batch_size]\n",
    "                hidden=hidden,  # [batch_size, hidden_size_d]\n",
    "                encoder_outputs=encoder_outputs,  # [batch_size, input_doc_len, hidden_size_d]\n",
    "            )\n",
    "\n",
    "            outputs[t] = pred\n",
    "\n",
    "            if np.random.sample() < teacher_forcing_ratio:\n",
    "                input = target_outputs[:, t]  # Size([batch_size])\n",
    "            else:\n",
    "                # Q: 2x softmax?\n",
    "                # Q: можно ли не брать софтмакс?\n",
    "                input = pred.softmax(1).argmax(1)  # Size([batch_size])\n",
    "\n",
    "        return outputs.permute(1, 0, 2)  # [batch_size, max_len, target_vocab_size]\n",
    "a = torch.zeros(3, 2, 5)\n",
    "a[0, :, 1] = 1\n",
    "a.permute(1, 0, 2).softmax(2).argmax(2)\n",
    "(\n",
    "    torch.tensor([-1, -2, -2, -3.]).softmax(0),\n",
    "    torch.tensor([1, 2, 2, 3.]).softmax(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3698d303-a509-41b9-8da5-0854c9d29fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd87fadf-e8c6-4fb8-a72f-a6f4596464cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# RU to EN\n",
    "\n",
    "embedding_dim = 30000\n",
    "hidden_size = 512\n",
    "rnn_num_layers = 1\n",
    "encoder = Encoder(\n",
    "    input_dim=tokenizer_en.get_vocab_size(),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size_in=hidden_size,\n",
    "    hidden_size_out=hidden_size,\n",
    "    dropout=0.5,\n",
    "    rnn_num_layers=rnn_num_layers,\n",
    "    bidirectional=True,\n",
    ")\n",
    "attention = Attention(\n",
    "    hidden_size_e=hidden_size,\n",
    "    hidden_size_d=hidden_size,\n",
    ")\n",
    "decoder = Decoder(\n",
    "    output_dim=tokenizer_ru.get_vocab_size(),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size_e=hidden_size,\n",
    "    hidden_size=hidden_size,\n",
    "    attention=attention,\n",
    "    dropout=0.5,\n",
    "    rnn_num_layers=rnn_num_layers,\n",
    ")\n",
    "seq2seq = Seq2Seq(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    device=DEVICE,\n",
    ")\n",
    "seq2seq = nn.DataParallel(seq2seq)\n",
    "seq2seq.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(seq2seq.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "\n",
    "all(all(param.is_cuda for param in model.parameters()) for model in (encoder, decoder, seq2seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f91cb229-8c53-4291-87db-95a9895d958d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Seq2Seq(\n",
       "    (encoder): Encoder(\n",
       "      (embedding): Embedding(30000, 30000)\n",
       "      (gru): GRU(30000, 512, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "      (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (decoder): Decoder(\n",
       "      (attention): Attention(\n",
       "        (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=1, bias=False)\n",
       "      )\n",
       "      (embedding): Embedding(30000, 30000)\n",
       "      (gru): GRU(30000, 512, batch_first=True, dropout=0.5)\n",
       "      (fc): Linear(in_features=512, out_features=30000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e36d2b9-e601-4ba3-be61-24b10fdbfdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seq2Seq] Epoch 1...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Seq2Seq] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mseq2seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss(outputs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), y_train)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mОшибка: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids:\n\u001b[1;32m--> 168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n",
      "File \u001b[1;32m~\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 212\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[1;34m(self, inputs, target_outputs, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m    208\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(max_len, batch_size, target_vocab_size, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfill_(target_sos)\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;66;03m# encoder_outputs ~ [batch_size, input_doc_len, hidden_size_d]\u001b[39;00m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;66;03m# hidden ~ [batch_size, hidden_size_d]\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m         encoder_outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m#         input = torch.empty(batch_size, device=self.device).fill_(target_sos).type(torch.int64)\u001b[39;00m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m target_outputs[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# batch of sos ~ [batch_size]\u001b[39;00m\n",
      "File \u001b[1;32m~\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 51\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m        inputs ~ [batch_size, padded_input_doc_len_in_batch]\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m        encoder h_n -> decoder h_0\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#.flip(1)  # [batch_size, padded_input_doc_len_in_batch, embedding_dim]\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     output, h_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru(embedded)\n\u001b[0;32m     53\u001b[0m     h_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h_n[\u001b[38;5;241m0\u001b[39m, :, :], h_n[\u001b[38;5;241m1\u001b[39m, :, :]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# если bi=1 и rnn_num_layers != 1 ?\u001b[39;00m\n",
      "File \u001b[1;32m~\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Jupyter Notebook\\venv\\lib\\site-packages\\torch\\nn\\functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'[Seq2Seq] Epoch {epoch + 1}...')\n",
    "    optimizer.zero_grad()\n",
    "    outputs = seq2seq(X_train, y_train)\n",
    "    loss_value = loss(outputs.permute(0, 2, 1), y_train)\n",
    "    print(f'Ошибка: {loss_value}')\n",
    "    loss_value.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), 1)\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e7d4f2-641f-4846-8a43-0767a5ce4c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальное значение: 29997\n"
     ]
    }
   ],
   "source": [
    "max_value = max(max(sub_array) for sub_array in X_ru)\n",
    "\n",
    "print(f\"Максимальное значение: {max_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25a006d-43d9-439b-95ee-88df1ac7bdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальное значение: 29990\n"
     ]
    }
   ],
   "source": [
    "max_value = max(max(sub_array) for sub_array in X_en)\n",
    "\n",
    "print(f\"Максимальное значение: {max_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa6c5c-beda-44ef-a447-e39b9a31c895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
